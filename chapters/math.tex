% !TeX root = ../main.tex

\chapter{LAMMPS 在申威平台上的并行实现}

\section{LAMMPS应用流程分析}

\subsection{LAMMPS 应用介绍}
LAMMPS（Large-scale Atomic/Molecular Massively Parallel Simulator）作为分子动力学计算领域内模拟体系涵盖最广的计算平台之一，由隶属于美国能源部的桑迪亚国家实验室进行开发和维护，可以进行多种体系的分子动力学模拟计算，包括生物大分子的结构微观模拟，合金等复合材料的热稳定性等机械性能。在材料力学中统计温度变化下势能，弹性常数的变化情况，并且提供了涉及多重体系的大量势函数，可以同时模拟出于固液气不同状态下的粒子行为，相比于其他分子动力学软件，例如GROMACS\cite{berendsen1995gromacs}，NAMD\cite{phillips2005scalable}，AMBER\cite{pearlman1995amber}，CHARMM\cite{brooks1983charmm}，拥有计算模型丰富，运行性能强大，上手容易等特点，软件代码迭代周期短，主要由 C++ 进行编写，其计算性能出色，已经在许多大型计算机上完成了不同体系下的多类大规模计算。针对并行计算的情况LAMMPS 采用了空间分解的方法将模拟体系划分为多个区域，并将子区域按需分配给不同的处理器进行计算。

LAMMPS 项目在20 世纪90 年代中期开始，直到2004 年给出了第一个开源版本，并且知道如今仍然以相当快的速度进行更新。截至 2021 年 4 月，最新的版本是8APR2021。LAMMPS 在提供大量势函数以及计算力场的基础上，同时也支持大量的硬件加速平台，并且针对这些平台额外编写了众多加速包进行支持，例如Nvidia GPU\cite{hong2009analytical}，AMD ROCm\cite{kuznetsov2019porting}，Intel CPU 以及Xeon phi\cite{chrysos2014intel}。在加速模块中也提供了许多并行方法可供参考，如CUDA，OpenCL，ROCm HIP，OpenMP， MPI 等。LAMMPS 本身也对通用 CPU 的多种向量化指令集提供了支持，包括SSE2\cite{bik2002automatic}，AVX256\cite{lim2018implementation}，AVX512\cite{cornea2015intel} 等。

\subsection{应用功能单元分析}
针对大规模分布式场景，LAMMPS以空间分解内存并行策略，来进行粒子模拟区域的划分，划分后的区域被称为网格（tiled/brick），不同维度的模型拥有不同的网格形态（二维、三维），目的是将网格内的粒子映射到并行进程中，以进行后续的粒子计算。对于稀疏的粒子分布模型，网格的分布会导致处理器负载不均衡而性能受限，这里需要对网格子域进行评估，选择合适的配置保证网格内粒子数量一致。在计算粒子间短程作用域中，进程在访问其自身网格的粒子数据之外，还需要访问其相邻子域的数据，其子域的通信范围限制在截断半径之内，配置周期性边界的模型内，可以通过子域的周期性复制进行描述。

 \begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{tied.png}
  \caption{LAMMPS 模拟区域划分}
\end{figure}

  对于记录粒子间的相对区域，最重要的方案是邻居列表（Neighbor List），每个处理器都会生成verlet样式的列表，其中枚举了在体系截断半径内的所有邻居粒子。这部分列表通常都会分配连续的内存块，并按需申请或释放，邻居列表在整个模拟期间并非一成不变，而是每隔数个时间步之后进行重构，邻域的截断距离为：
  
\begin{equation}
R_n = R_f + \Delta s
\end{equation}

其中$R_n$定义为计算短程力势的最大截断值，$\Delta s$为重构列表时的系数，当粒子坐标发生$\Delta s/2$的偏移时，重构指令会被触发。过程会将进程原本截断半径内的粒子转移到新的处理进程中，在周期性边界条件判定之后，进程会对其划分之后的网格粒子进行空间排序，提高后续时间步受力计算效率，并进一步利用cache的局部性特征。

进程间基本的通信模式是将自身的粒子信息，转发到其临近的进程，完成ghost粒子的更新，这类通信模式称为前向通信。将ghost粒子信息从其他进程发送到当前进程，来进行数据信息的统计累加，这类通信称为后向通信。其中前向通信在所有计算模式中都有涉及，而后向通信只在使用半邻接表的场景下使用，此时粒子对<i,j>受力只进行单次计算。

 \begin{figure}[h]
  \centering
  \subfigure[]{
      \centering
      \includegraphics[width=0.4\textwidth]{nlist1.png}
  }
  \hspace{0.5in}
  \subfigure[]{
      \centering
      \includegraphics[width=0.4\textwidth]{nlist2.png}
  }
  \caption{LAMMPS 邻居列表}
\end{figure}

 作为LAMMPS计算中最基本的运算流程，单个时间步的执行描述了势模拟在最小时间单位下的执行流程。verlet方法提供时间步的底层执行逻辑，并衍生出能量最小化的弛豫与动态执行两类时间步类型。单个时间步执行前，记录标志与受力参数会被部分初始化，在执行的各个阶段，允许使用时间步长动态进行矢量速度及位置的更新，并同时判断在当前位置是否需要重建邻居列表。对于周期性边界的场景，还会将坐标在边界外的粒子进行调整。识别已经离开处理器子域的粒子，并与相邻处理器进程通信重建邻居列表，在计算应力过程中，全局virial计算涉及已存的粒子坐标与受力，复杂度为O(n)，按照分子拓扑结构（键，二面角等）执行。在时间步执行末尾，后处理阶段会考虑输出信息的配置，包括结果打印，日志与当前时间步的快照信息等。

  \begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{tied_comm.png}
  \caption{LAMMPS 粒子通信模式}
\end{figure}

\subsection{并行编程模型}
RM不稳定性演化中eFF电子力场的核心计算，在LAMMPS以单独势函数包提供，通过自身并行框架提供运行实例。本节将基础计算场景进行分解，在SW26010上完成通用并行计算框架，这里按照LAMMPS的底层执行功能，将应用划分为数据配置I/O，系统初始化，进程通信与核心计算模块，进一步根据计算特征划分代码执行逻辑，将子任务映射到主从核执行单元。主核（MPE）作为系统控制核心，负责整体任务的分配与调度，数据I/O，进程间通信与少量辅助计算任务，从核（CPE）细粒度提供众核计算完成高负载任务的执行，与核心计算的加速工作，应用模型的分解与任务分配如图3.x所示。

应用初始化模块主要涉及粒子数据文件的I/O加载，物理模型的构建，进程内粒子结构的初始分布，并按照负载划分粒子区域，在此运行阶段不涉及复杂计算，因此分配此模块任务为主核完成。对于电子力场势核心计算，在激波穿越晶粒完成演化前，单个时间步下的进程邻居列表重构，ghost粒子数据的复制与统计及系统总能量的累加，这类分析与任务调度的执行分配主核完成。对于密集型的计算任务，粒子对势<i,j>的迭代访问，激波在模型内传递的结构演化，系统应力的计算，总能量与矢量速度的统计与修正等任务分配给从核执行。

\subsection{应用编程框架}
针对进程级并行的消息传递与任务分发，本工作集成LAMMPS的MPI并行执行框架，遵循正交并行分解策略提供空间域。实现并行化的关键问题，是要减少在更新受力数据时进程的竞争关系。在模拟交互时，大规模粒子的引入，总会使得在多处理器在更新相同粒子时导致竞争。LAMMPS并行框架通过访问重构，将列表拆分在并行计算时不同重叠。对于粒子只在单一线程上计算一次，并只在当前计算域进行更新，采用原子操作并在单个线程中使用副本数据结构解决规约冲突。

细粒度的线程级并行交互，神威平台支持以OpenACC 2.0标准为基础的SWACC并行框架域Athread并行编程模型。前者以并行指导语句为基础，通过在不破坏原有程序逻辑的基础上，插入预处理语句提供并行手段与能力，计算区域通常为粒子循环，执行模型将核心计算区域卸载分解为到加速器单元。在这类加速执行场景中， 主核与加速单元的模型通常是分离的，使得调用者自身并不需要对加速设备结构深入涉及，即可完成计算移植加速，但这也反而无法充分使用加速单元的计算性能。Athread并行框架提供充分的从核运行接口，能够精确控制从核线程运行单元。在核组运行时利用Athread框架完成主核与从核核组的交互操作，包括内存通信设备访问和执行调度等功能，细粒度控制硬件与指令的执行行为。

本节以神威超算多级编程模型为基础，设计了$Li\H_2$电子力场势的分子动力学多级并行框架，涵盖进程级的区域计算并行与线程级的双层粒子循环分解并行。模型内部作为计算的整体域，在激波发射前不同粒子在各自区域内均匀排布，对于三维或更复杂的三维模型，需要按照粒子物理坐标分解排布，对于周期性边界，调整边界位置以适应所有粒子。分解的网格尺寸体积随粒子分布动态调整，三维区域划分意味者需要考虑x、y、z笛卡尔轴的非重叠子域结构。

进程级的MPI并行，会将单个进程映射到划分域上的数个网络，网格数量由区域粒子密度决定。作为网格最基本的划分阶段，在应用启动初期，网格会以静态的方式均匀分解并映射到MPI进程。随着模拟进程中网格的划分，子区域的粒子密度会逐渐变化，导致负载不均衡，整体计算速度受最慢的处理器执行的限制。这里在模拟运行时，根据子域的体积与粒子分布，优化网格的划分情况，动态划分MPI进程映射的粒子区域。单个进程以子域区间分布作为输入，通过电子力场势函数计算，获得单个时间步后的粒子坐标与应力结果。此流程内所有计算进程参与各自子域计算模拟，时间步结束后，进行系统总能量的拷贝汇总，对局部粒子情况进行修正。该阶段所有时间步运行结束，广播并进行MPI进程间的热力学状态与模拟阶段的汇总，包括迭代与收敛能量梯度，至此完成单次电子力场分子动力学模拟流程。

整个模拟界面划分为单个计算子域后，粒子间势函数的计算由密集计算的从核加速单元负责，搭载MPI进程的主核负责从核任务的分发与结果统计，线程级的从核阵列对计算子域的粒子进行划分。势函数中心计算，对势粒子迭代中，将当前核组所属的粒子区域划分为8x8，分配给对应线程。该线程维护子域粒子的数据，分析数据结构，数据类型为双精度浮点，打包数据结构，同步发送给线程LDM局存，由于单个从核对全部子域粒子要分批计算，不同体系规模的计算效率不同，从而影响并行性能。这里测试在单次计算规模为K的最大并行性能，同时对邻居粒子数据打包访问，在从核执行内部，本文还将利用向量化指令操作与寄存器特征，进一步挖掘线程层面的处理器并行性能。

\subsection{LAMMPS 工作流程介绍}
LAMMPS 的主要工作流程大致可分为三部分：初始化过程，模拟与计算，数据的后处理过程。

\subsubsection{初始化过程}
首先生成输入数据文件，这里可以根据计算模拟的体系与类型来控制数据文件，可以使用 matlab 进行数据文件的生成，其中包括体系中粒子的规模，类型个数，所分布区域的范围，然后为每个粒子设置相关参数，包括初始化位置坐标，自旋类型，编号及带电量等。接下来再编写输入配置文件，可以在配置文件中通过编写计算参数来模拟不同的计算过程，主要包括设置边界条件，时间步长，续算文件的控制，还可以描述输出结果的频率与所在模拟过程的系综。

\subsubsection{模拟与计算过程}
通过输入文件将体系中的粒子数据读入并形成完整的初始体系，首先利用Minimize 过程进行能量最小化的步骤，使其达到一个能够模拟稳定的初始状态，然后通过设置温度来为体系中所有粒子赋一个初速度，粒子开始无规则运动并进行一定时间步的弛豫过程，使粒子出于一个分散的状态，之后通过对体系边界的收缩产生激波使粒子随激波进行定向运动。在整个模拟过程中以牛顿定律为基础，通过对粒子间作用力的变化产生加速度与速度的改变，最后再把每个周期粒子信息记录下来，形成完整的运动轨迹。通过电子力场势函数完成对粒子间作用力的计算是最复杂也最耗时的部分，而在并行计算的过程中，通信的时间也占用了不小的比例。在完成每个时间步作用力的计算后，由于粒子的坐标位置不断变化，粒子对应的邻接表信息也会发生改变。过大的时间步长会使模拟的计算结果发散，所以这里选择△t = 0.005fs 作为整体计算的时间步长，而完成整个模拟计算则至少需要几十万的时间步。

\subsubsection{后处理过程}
在完成各个阶段的计算之后，LAMMPS 会给出多个不同类型的输出文件来对整个模拟过程进行量化的描述，以供后续的处理和分析。其中包括结果日志文件，结构文件，不同粒子的轨迹文件，续算文件等。再通过可视化工具对计算结果进行基本的分析处理，并验证结果的正确性。可视化工具包括OVITO，VMD等。这里能够获得在不同的模拟阶段不同类型粒子结构的运动轨迹和状态，最后通过 matlab 或 python 脚本完成粒子的多角度统计和分析。

 \begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{lammps_flow.png}
  \caption{LAMMPS 模拟流程图}
 \end{figure}

 \subsection{输入文件结构}
 LAMMPS进行多体系粒子物质模拟流程，通过对输入文件（input script）解析，获取模拟体系的基本结构配置，进而读取粒子文件（data file）完成模拟。输入文件按执行功能分为 Initialization、Compute、Output三个模块。Initialization负责提供模拟体系内的基本配置与初始化信息，包括模拟维度、边界条件、粒子类型参数与拓扑信息等。在基本配置确定后，Compute模块负责完成单一或组合处理流程的模拟，提供弛豫、参数更新等作用。Output则在模拟结束后，指定输出文件的信息，后续可借助第三方可视化软件OVITO等完成图形输出。
 \begin{table}[]
  \centering
  \caption{LAMMPS input file parameter}
  \renewcommand{\arraystretch}{1.5}
\begin{tabular}{lll}
\hline
Module                          & Function        & Command                                                                                                                                                            \\ \hline
\multirow{3}{*}{Initialization} & Style           & \begin{tabular}[c]{@{}l@{}}atom\_modify, atom\_style, boundary,dimension, newton, \\ processors, units\end{tabular}                                                \\ \cline{2-3} 
                                & Atom definition & \begin{tabular}[c]{@{}l@{}}create\_atoms, create\_box, lattice, read\_data, read\_restart, \\ region, replicate\end{tabular}                                       \\ \cline{2-3} 
                                & Force fields    & \begin{tabular}[c]{@{}l@{}}angle\_coeff, angle\_style, bond\_coeff, bond\_style, \\ dielectric, dihedral\_coeff\end{tabular}                                       \\ \hline
\multirow{5}{*}{Compute}        & Run             & \begin{tabular}[c]{@{}l@{}}communicate, dipole, group, mass, min\_modify, min\_style, \\ neigh\_modify, neighbor, run\_style, set, timestep, velocity\end{tabular} \\ \cline{2-3} 
                                & Fixes           & fix, fix\_modify, unfix                                                                                                                                            \\ \cline{2-3} 
                                & Compute         & compute, compute\_modify, uncompute                                                                                                                                \\ \cline{2-3} 
                                & Actions         & \begin{tabular}[c]{@{}l@{}}dump, dump\_modify, restart, thermo, thermo\_modify, \\ thermo\_style, undump, write\_restart\end{tabular}                              \\ \cline{2-3} 
                                & Miscellaneous   & \begin{tabular}[c]{@{}l@{}}delete\_atoms, delete\_bonds, displace\_atoms, displace\_box, \\ minimize, run, temper\end{tabular}                                     \\ \hline
Output                          & Write           & clear, echo, if, include, jump, label, log, next, print, shell                                                                                                     \\ \hline
\end{tabular}
\end{table}


\section{函数分析}

\subsection{代码框架}
图 3.1 给出了 LAMMPS 中 eFF 势函数的计算过程, 整个计算包含两次 for 循环，第一层for 循环迭代体系中每个一个粒子，作为计算中的i，第二层循环则遍历迭代每个i 的邻居粒子，其中粒子由邻居列表给出，在确定粒子间距在对应的截断半径内后，开始进行粒子间受力的计算，在计算中由于粒子类型的不同，半径和带电量都会有所差别，并且这里会采用不同的计算方法，之后会给出此轮计算的最终受力，分别累加到 i，j 粒子上。

\subsection{数据结构}
图 3.2 是 LAMMPS 中 eFF 计算中需要的数据结构，大致分类两类，第一类是粒子本身保存的数据，例如粒子坐标，类型，带电量等，这类数据大多是在计算输入文件中直接给出的，第二类则是在计算中体系生成的邻接表相关的信息。


\begin{table}[]
   \centering
  \caption{electron Force Field potential 基本数据结构}
\begin{tabular}{@{}l|l|l@{}}
\toprule
类                      & 数据变量         & 含义                                                                                           \\ \midrule
\multirow{6}{*}{atoms} & coordinate   & \begin{tabular}[c]{@{}l@{}}当前进程所拥有的粒子位置信息, 第一维代表粒子\\ 序号, 第二维分别代表 x，y，z 三维上的位置信息\end{tabular} \\ \cmidrule(l){2-3} 
                       & electricity  & 不同类型和序号粒子的带电量                                                                                \\ \cmidrule(l){2-3} 
                       & force        & \begin{tabular}[c]{@{}l@{}}进程中不同粒子的受力信息, 第一维代表粒子\\ 序号, 第二维分别代表 x，y，z 三维上的受力\end{tabular}     \\ \cmidrule(l){2-3} 
                       & type         & 对应不同粒子的类型信息，例如: 氢原子，电子                                                                       \\ \cmidrule(l){2-3} 
                       & spin         & 对应不同粒子的自旋类型，例如: 原子核，电子                                                                       \\ \cmidrule(l){2-3} 
                       & nlocal       & 当前 MPI 进程所拥有的粒子数                                                                             \\ \midrule
\multirow{4}{*}{lists} & ilist        & 对应粒子的索引位置                                                                                    \\ \cmidrule(l){2-3} 
                       & inum         & 邻接表中 I 粒子数量                                                                                  \\ \cmidrule(l){2-3} 
                       & numneigh     & 邻接表中的每个 I 粒子对应的 J 粒子数量                                                                       \\ \cmidrule(l){2-3} 
                       & firstneigh   & 每个 I 粒子对应的 J 粒子序号                                                                            \\ \midrule
\multirow{2}{*}{pairs} & eatom        & 每个粒子的能量                                                                                      \\ \cmidrule(l){2-3} 
                       & cutsq        & 不同类型粒子直接的截断半径                                                                                \\ \midrule
force                  & newton\_pair & 牛顿第三定律参量标志位                                                                                  \\ \bottomrule
\end{tabular}
\end{table}

\subsection{核心算法流程}
在分析模拟体系交互场景中，对于压力、温度等粒子数据采用统计学手段进行计算，利用势能来分析系统整体变化，构造相关势函数来推导粒子间的时空变化，这种手段能够规避复杂场景下的算法的额外描述。电子力场势函数eFF基于波色体系，提供对原子电子结构的多维作用计算，包括电离、碰撞等，其势函数为：
\begin{equation}
U(R,r,s) = E_{NN}(R) + E_{Ne}(R,r,s) + E_{ee}(r,s) + E_{KE}(r,s) +E_{pauli}(\uparrow\downarrow,s)
\end{equation}
其中$E_{KE}$代表电子本身的动能，s代表电子波包半径，根据海森堡原理，当电子拥有较小的波包半径时其动能更高。$E_{NN}$，$E_{Ne}$和$E_{ee}$分别代表原子核之间，原子核和电子之间，以及电子间的库伦力。由于电子的特殊性，这里使用电子 pauli 排斥能 Epauli对电子进行校正。
在进行电子势能统计时，需要对波包整体进行积分，erf(x)提供了统计的误差项分析，也引入的复杂计算。可以看出，在eFF势函数的计算过程中，除了基本的四则累加运算，自身还带有开方、指数函数来对不同粒子形式的势能变化进行描述，超越函数的处理会使势函数的计算量增大，意味着计算部分的优化也成为了提升性能的关键手段。

\begin{algorithm}[h]
  \SetAlgoLined
  \KwIn{$\vec{X}$，Q：整个体系中不同粒子的坐标和带电量\newline
    S，R，T：粒子自旋，波包半径，类型\newline
    L：邻接表\newline
    n：体系中的粒子总数\newline
    $r_{cut}$：截断半径
  }
  \KwOut{$\vec{F}$：粒子受力\newline
    E：体系中的eFF势能
  }

  \For {$i\in[0...n)$}{
      \For {$j\in L_j$}{
        $\vec{r_{ij}} \leftarrow \vec{x_i}-\vec{x_j}$\;
        $ r_{ij}^2 \leftarrow r_{ij}·r_{ij}$\;
        \If {$r_{ij}^2 < r_{cut}^2$}{
          $ dspline_i \leftarrow cutoff(r_{ij})$\;
          \uIf {$S_i = 0 \&\& S_j = 0$}{
            $fpair,ecoul \leftarrow ElecNucNuc(Q,r_{ij})$\;
          }\uElseIf{$S_i = 0 \&\& abs(S_j) = 1$}{
              $fpair,ecoul \leftarrow ElecNucElec(Q,r_{ij})$\;
          }\uElseIf{$abs(S_i = 1) \&\& S_j = 0$}{
            $fpair,ecoul \leftarrow ElecNucElec(Q,r_{ij})$\;
          }\ElseIf{$abs(S_i = 1) \&\& abs(S_j) = 1$}{
            $fpair,ecoul \leftarrow ElecElecElec(Q,r_{ij})$\;
          }
          $fpair \leftarrow fpair·dspline$\;
          $E \leftarrow ecoul·dspline$\;
          $\vec{f} \leftarrow SmallRForce(fpair,\vec{X})$\;
          $\vec{F_i} \leftarrow \vec{F_i}+\vec{f}$\;
          $\vec{F_j} \leftarrow \vec{F_j}+\vec{f}$\;
        }
      }
  }
  \caption{electron Force Field 势函数核心计算}
  \label{algo:algorithm1}
\end{algorithm}

\section{主核移植}
申威处理器作为并行处理器，计算的并行实现很大程度上需要依赖从核得高效计算和并行，但主核绝非出于次要的位置。主核自身功能与通用处理器类似，能够执行独立的进程，这就意味着在并行之前，我们可以先将程序移植到主核上，主要目的不仅仅是能够作为计算并行和优化的起点，更主要的是可以将其作为依据，确保计算在移植前和优化后的结果一致。

申威处理器主核具有类似于通用处理器的运行流程，这样移植到主核上时程序本身基本可以不做修改。而主要工作就是修改编译环境，其中核心的是替换编译器，设置编译和链接路径，增加对应的编译参数，保证计算的正确性。

\section{eFF 势函数计算在申威处理器上的并行实现}
\subsection{程序移植}
LAMMPS 作为一款分子动力学计算平台，其主要设计模型主要是使用 C++ 语言实现，这不仅在支持众多特性的同时，也使整个模型脉络和层次更加清晰，这当前几乎所有分子动力学平台一致采用的方案。但对于申威处理器来说，却并不是一个优势，反而成为了移植并行的一个障碍。从核编译器无法直接进行C++ 代码的编译，而进行并行和优化的前提正是高效利用从核进行计算。所以实现并行的第一步就是要把热点计算的部分重新以 C 语言实现，并剔除 C++ 中相关的语言特性，诸如类和对象，虚函数等。利用申威处理器的交叉编译来完成编译链接，为此设计了一个用于数据传递的结构体，其中包含了表 K 中的列出的所有计算信息。

除此之外，由于在访问结构体，数组，向量等数据时，不对齐的Load/Store操作会引发不对齐访存异常，主核在收到异常会将指令拆分，但性能会大幅降低，而从核在引发异常后，程序会直接退出。这里有两种解决方法，第一种是在编译时加入-faddress\_align=N 编译选项，保证所有结构体，数组的首地址都以N字节对齐；第二种是在创建数组，结构体时加入\_\_attribute\_\_(aligned(size)) 进行手动对齐。前一种方法采用一种全局配置的方法解决访存对齐的问题，但同时会使编译器对其他无关访存操作的结构体，数组的首地址进行对齐处理，所以我们为了保证访存性能，选择后一种方案进行手动对齐。

\subsection{任务划分}
在进行 eFF 势函数计算中，绝大部分计算都集中在对两体势计算的过程中，也正因为如此对势函数的计算过程需要在从核中进行并行实现，算法核心框架如图 K 所示。整个势函数计算主要由两层迭代构成，外层循环遍历整个体系的粒子，计算规模由体系自身的粒子总量决定，可达千万量级。内层for 循环迭代粒子自身周边的邻居粒子，由邻接表指定，规模与体系内粒子密度和截断半径相关，一般来说在数百左右。

利用从核进行计算并行的关键，就是要对这两层循环进行拆分，由于外层循环规模要明显大于内层循环，所以将外层循环拆分实现从核级并行是最优的方法。但由于分子动力学中牛顿第三定律的引入，导致在更新中心粒子受力的同时，不可避免地需要同时更新邻居粒子的受力情况，这样一来就会加剧计算时的数据依赖和存储空间限制等问题。下面会提出多种并行方案，通过平衡并行粒度，存储空间使用和计算量来找出对于计算 eFF 势函数更合适的并行方案。

\subsection{内层循环并行}
根据对势函数计算的分析，整体计算的并行主要是依赖于对内外层循环的拆分，并均匀分配到从核上。但由于牛顿第三定律的引入，在进行中心粒子的计算时，周围截断半径内的粒子受力信息也一并会更新，这使得在并行时数据依赖成为了一个棘手的问题。

为了解决数据依赖的问题，这里选择了只对内层循环进行拆分，就是对单个粒子的邻居粒子计算进行拆分，并在每个周期对从核分配 N 个粒子对的计算，这样为了保证对于每个从核能够在一个执行周期内时间基本一致。

当每个从核进行 i 粒子与 N 个邻居粒子的粒子对的计算时，由于在这 N 次计算中i 粒子的坐标，电荷量等粒子信息会被多次调用，所以这些信息在计算时会一直保存在从核LDM 中，直到当前粒子计算完成，这就避免了从核频繁访问主存带来的开销。

当每个从核对同一个进行所有邻居粒子的计算时，由于需要同时对i 粒子受力情况等信息进行更新时，就会带来写依赖的问题，不同从核会在进行 N 个粒子对的计算完成之后，进行并发地对i 粒子受力信息的更新，从而引发数据写写依赖的问题。这里选择在计算受力信息完成后不立即更新i 粒子，而是只保留每个从核计算后的增量，并保存在LDM 中，只有当整个粒子与所有邻居粒子的计算完成之后，才进行对受力情况的累加。这里选择0 号从核作为受力信息变化量的接收方，每个从核在累加受力情况之前，需要对从核间进行同步，以此来保证数据的一致性，之后每个从核会逐步将受力信息累加到0 号从核，在最终受力计算完成后，只由 0 号从核完成受力结构的写回，实现整个 i 粒子部分的计算。

除了不同从核在更新i 粒子受力情况时会出现写写依赖，在单个从核完成对i 粒子的计算之后，进行下一个 i 粒子的计算时会出现读写依赖。这是因为上一个粒子的计算结果是由 0 号从核单独写回的，而从核访存需要数百个时钟周期来进行，速度要远低于计算指令的执行速度，就会出现在从核在进行下一个粒子的计算时，上个粒子的结果还没有写回，造成读写依赖。这里需要在每个从核在完成对同一个i 粒子的计算后，等待0 号从核完成粒子信息的写回，进行从核核组内的同步，消除读写数据依赖。还有一个注意的问题就是写回数据的连续性。由于牛顿第三定律的引入，受力等写回信息会被不同从核多次进行更新，这也就要求每次在访存的时候不能只对粒子坐标，带电量等不进行写回的数据读取，还需要读取受力等只写数据，并将此轮计算结果进行累加，保证数据的连续和正确。

\subsection{副本规约方法}
对于内层循环并行方法，将第 i 层粒子与邻居粒子之间的计算实现了并行，在解决局部数据依赖的同时，一定程度上实现了从核级并行。但最大的问题就是每个粒子的邻居粒子数量十分有限，只有不到一千个，这在进行计算划分时从根本上限制了其并行粒度。为了提高计算的并行粒度，副本归约的方法采用对外层循环进行拆分，由于第i 层循环次数与体系规模有关，在体系中计算粒子的数量达到百万乃至千万的量级，并行粒度要远大于内层循环并行的方法。副本归约的方法每次将 N 个第 i 层粒子的计算赋给单个从核，对于 0 号从核计算 0 到第N-1 粒子，第N 到第2N-1 的粒子分给1 号从核，按照这个方法均匀分配了64 个从核之后，剩下的粒子依旧按照上述次序划分，知道所有粒子均划分完成。接下来讨论 N 的取值情况，得到一个合适的 N 值要考虑多方面的因素，在不同计算体系和规模下 N 的取值也可能是不同的。这里给出能够影响 N 取值的因素以及如何平衡 N 的取值。由于要保证一定的从核并行计算粒度，并且在当分配粒子数过少时对于访存会产生更大的开销，但又当 N 值过大的时候，每次所保存在LDM 中的粒子信息又会过大，由于存储空间的限制，又无法保证大规模计算的进行，并且在最后一轮的计算中，从核间计算的分配又会更加不均匀，反过来影响计算性能。

在副本归约方法中，数据写依赖的问题会变得更加严峻，因为对外层粒子循环进行拆分会对中心粒子和邻居粒子同时进行多次更新，为了解决存在粒子被多个从核同时更新造成写写依赖的问题，每个从核LDM 都会开辟一份单独的空间，在从核计算完受力等信息后，不直接对粒子进行写回，而是将结果写到临时空间内，等到从核此轮计算全部结束后，由0 号从核完成计算结果的累加，并单独写回主存。

这种方法虽然能够提高并行性，但由于需要为 64 个从核在 LDM 中单独开辟一个副本，这对存储空间来说是一个极大的消耗，在计算规模过大的时候，这无疑是不可接受的。下一章会介绍一种通过平衡计算量，并保证从核计算并行度和计算规模的方法。
